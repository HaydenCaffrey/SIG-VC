# SIG-VC
### This repo is the official implementation of the paper "SIG-VC: A Speaker Information Guided Zero-Shot Voice Conversion System for Both Human Beings and Machines"   
### The paper can be found at https://arxiv.org/pdf/2111.03811.pdf

Following the steps below to reimplement the model or train on your own custom dataset.
## 1. Download Pre-trained Models
Download the pre-trained bottleneck feature extractor and put it under bn_extractor/acoustic_mdl/  
https://drive.google.com/file/d/1y6c2z-6EbtuCz5tHev4KN9Fs69bCaj4_/view?usp=sharing

Download the pre-trained speaker embedding extractor and put it under conversion/sv_model/save_model/ECAPA-TDNN/  
https://drive.google.com/file/d/1qsjR7mzHrpVqbNrt_QvtTLTmWo52XIf_/view?usp=sharing

Download the pre-trained voice converesion model and put it under conversion/aishell_twostage/ckpt/  
https://drive.google.com/file/d/1FVr1fpncP5cpSMQYQaG0yhrScDmaEb1p/view?usp=sharing

## 2. Environment Setup
### compile and install kaldi
Resources that might help:  
    kaldi build up: https://www.kaldi-asr.org/doc/build_setup.html  
    Cygwin: https://groups.google.com/g/kaldi-help/c/C2tcoAsP9Uo  
```
#Bash

#download kaldi
git clone https://github.com/kaldi-asr/kaldi.git kaldi --origin upstreamcd
kaldi

#compile kaldi
#go to 'tools' directory under the kaldi repository
cd tools
#check whether the system dependencies are installed, if not, install them according to the prompts
./extras/check_dependencies.sh

#install kaldi
#go to 'src' directory under the kaldi repository
cd src

#run the following command (they are also in the INSTALL file)
./configure --shared
make depend -j 8
make -j 8
```
n.b. Remember to edit the KALDI_HOME variable in bn_extractor/path.sh to point to the kaldi repository on your machine

### Install Anaconda Virtual Environment
```
#Bash
conda env create -f pytorch.yaml
```
Now you have a anaconda virtual environment named "pytorch", to enter this virtual environment, run:
```
#Bash
conda activate pytorch
```

## 3. Data Preparation
### Prepare the training and testing files
All the training data should be put under the path conversion/training_data/
All the test data should be put under the path conversion/test_data/
For inference, please copy the wav files of source speaker under the path conversion/test_data/source, and copy the wav files of the target speaker under the path conversion/test_data/target/
The inference code with generate converted speech for each source-target pair.
Please refer to the following example structure:
```
conversion/training_data/
    t1.wav
    t2.wav
conversion/test_data/
    t3.wav
    t4.wav
    /source/
        t3.wav
    /target/
        t4.wav
#the following directory will be automatically generated by the provided code
conversion/bnf
conversion/mel
conversion/emb
```

### Extract Bottleneck Features
Under the path <root>/conversion/ (<root> is where you put this repository)
```
#Bash
bash extract_bnf.sh training_data bnf
bash extract_bnf.sh test_data bnf
```

### Extract Mel-spectrogram
```
#Bash
#extract mel-spectrogram of training data
python mel.py --mode train

#extract mel-spectrogram of test data
python mel.py --mode test
```

### Extract speaker embeddings
```
#Bash
python embed_extraction.py
```

## 4.Train the model
### Finetune from the pre-trained model (pre-trained on AISHELL-3)
```
#Bash
CUDA_VISIBLE_DEVICES=<n> python train.py --restore_step 201000
```

### Train from scratch
```
#Bash
CUDA_VISIBLE_DEVICES=<n> python train.py
```

## 5.Inference
Run the following commands, the converted speech will be under the path conversion/syn_wav/
```
#Bash
#372000 is the step when the model converges, you can refer to aishell_twostage/ckpt/ to find the converging step 
CUDA_VISIBLE_DEVICES=<n> python test.py --test_step 372000

#convert the generated mel-spectrogram to wav
python melgan-80-fast/synthesis.py ./syn_mel ./syn_wav
```
